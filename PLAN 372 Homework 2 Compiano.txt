# PLAN 372 Homework 2# I have chosen to work with the restaurant data set# First, I downloaded the csv and put it into a hw 2 file# I made that file my working directory and added a new blank R script file,# which is the file I am currently working in# load librarieslibrary(tidyverse)library(lubridate)# I loaded the csvrestaurant_inspections = read_csv("hw 2/data/restaurant_inspections.csv")# I read a selection of the data to view the different column headersView(restaurant_inspections[1:20,])# Question 1: histogram of overall inspection scores# First, I wanted to check whether there were any missing scores, since this# could skew the analysissum(is.na(restaurant_inspections$SCORE))# Since there weren't any, I went ahead and created the histogram without manipulating the data, # since this was just an overall distributionOverall_Scores = restaurant_inspections$SCOREhist(Overall_Scores)# Question 2:# There are quite a few missing restaurant opening dates so, I wanted to # identify those and remove them# First, I found how many restaurants were missing opening datessum(is.na(restaurant_inspections$RESTAURANTOPENDATE))# I found that there are 296 missing opening dates. Since they are missing, # I want to remove them from my analysisdata2 = filter(restaurant_inspections, !is.na(RESTAURANTOPENDATE))# Next, I had to mutate the opening date column so that the data is in a# recognizable form.data2$RESTAURANTOPENDATE = ymd_hms(data2$RESTAURANTOPENDATE)# I then pulled the year from the opening dates since month or date were not # relevant. From there, I can analyze the relationship between inspection scores# and the year the restaurant was openeddata2$opening_year = floor_date(data2$RESTAURANTOPENDATE, unit="year")# Before plotting my data, I made a new dataset that only contained the opening# year and averaged the scores per year. This consolidated scores so there was# only one mean score per recorded year.scores_by_opening = group_by(data2, opening_year) %>%  summarize(SCORE=mean(SCORE))# I plotted the data.ggplot(scores_by_opening, aes(x=opening_year, y=SCORE)) +  geom_line()# Both the starting year, 1986, and the most recent year, 2022, have very high# average inspection scores at approximately 99.5 and 100, respectively. Scores # decrease until about 1997, where the dataset reaches a minimum average of# about 95.8. From 1997-2000, there are multiple irregular peaks. Between about# 2003 and 2021, the data levels out with approximate scores of 96.7-97.3.# Question 3: # Just like the previous questions, I wanted to make sure there were no missing # city "values"sum(is.na(data2$CITY))# Since there were none, I went ahead and extracted all of the unique CITY values# to see what the possibilities are.unique(data2$CITY)# I saw that there were 31 city names, many of them duplicates based on format# or spelling # I recoded all of the cities individually to consolidate the city name# variances into one name.data2$CITY = recode(data2$CITY, "FUQUAY VARINA"="FUQUAY-VARINA", "FUQUAY-VARINA"="FUQUAY-VARINA",                     "Fuquay Varina"="FUQUAY-VARINA", "Fuquay-Varina"="FUQUAY-VARINA",                     "MORRISVILLE"="MORRISVILLE", "Morrisville"="MORRISVILLE",                     "MORRISVILE"="MORRISVILLE", "RTP"="RTP", "RESEARCH TRIANGLE PARK"="RTP",                    "CARY"="CARY", "Cary"="CARY", "Apex"="APEX", "APEX"="APEX",                     "HOLLY SPRINGS"="HOLLY SPRINGS", "Holly Springs"="HOLLY SPRINGS",                     "HOLLY SPRING"="HOLLY SPRINGS", "ZEBULON"="ZEBULON",                     "Zebulon"="ZEBULON", "WAKE FOREST"="WAKE FOREST",                     "Wake Forest"="WAKE FOREST", "RALEIGH"="RALEIGH", "Raleigh"="RALEIGH",                     "GARNER"="GARNER", "Garner"="GARNER", "ROLESVILLE"="ROLESVILLE",                     "ANGIER"="ANGIER", "WILLOW SPRING"="WILLOW SPRING",                     "NORTH CAROLINA"="NORTH CAROLINA", "KNIGHTDALE"="KNIGHTDALE",                     "WENDELL"="WENDELL", "CLAYTON"="CLAYTON", "NEW HILL"="NEW HILL")# I did all of them individually to ensure all data was included and standardized# After recoding, I made a new dataset with just cities and took the mean score # for each to see if on average, inspection scores varied per cityCITY_RECODE = group_by(data2, CITY) %>%  summarize(SCORE=mean(SCORE))# Before moving forward, I double checked that there were only 16 different cities# rather than the 31 pre-recode.unique(CITY_RECODE$CITY)# It all looks good, so then I viewed the new dataset to interpret the average scores.CITY_RECODE# There are not any significant variances between the cities, but two-thirds of # the cities have an inspection average of 96 (5 cities) or 97 (7 cities). The # exceptions to this are Holly Springs and RTP who have average scores of 98, # New Hill who had an inspection score of nearly 100, Wendell who had an average# score of 95, and Angier and Zebulon who both have average scores of 94.# Question 4:# Again, before starting I wanted to check if there were any na values for inspectorssum(is.na(data2$INSPECTOR))# There were not any so I went ahead and viewed the different inspector namesunique(data2$INSPECTOR)# There were 37 inspectors# Then, I made a new dataset with just the inspectors and their average scores# to see if there are any major variances inspector_scores = group_by(data2, INSPECTOR) %>%  summarize(average_score=mean(SCORE))# Average scores are around mid-to-high 90's with the exception of Thomas Jumalon# who had an average score of 89. He was the only inspector to have an average# not in the 90's# Question 5:# I wanted to check the sample size of the datasets used for questions 1-4 to# see if it could explain any irregularities# Sample size of scores per opening yearyear_samplesize = group_by(scores_by_opening, opening_year) %>%  summarise(number=n())# The first and last recorded year both had a sample size of 1, making it reasonable# to assume the irregular high averages of about 99.5 and 100 can be explained# by sample size. The sharp decrease and minimum around 1993 and 1994 corresponds# with a sample size of 17, which is a good bit lower than the sample sizes of# years before and after. Additionally, the irregular distribution until 2003 and# the leveled-out distribution post-2003 roughly corresponds with typically small# sample sizes (double digits) followed by significantly larger sample sizes # (triple digits). The large sample sizes likely contribute to the regularity of the# latter data due to the decrease in margin of error.# Sample size of each citycity_samplesize = group_by(data2, CITY) %>%  summarize(number=n())# I chose data2 as my dataset because my CITY_RECODE dataset consolidated all the# city scores into one average, meaning the instance count/sample size would just# be one. data2 has every instance with the recoded names.# Holly Springs, RTP, Wendell, New Hill, Zebulon, and Angier were my 6 "exception"# cities. Their sample sizes were 107, 2, 35, 2, 50, and 1. Additionally, there# were other cities with small sample sizes, such as Clayton (4) and North # Carolina (1), but they fell within the typical average. As a result, it's  # possible sample size impacted RTP, New Hill, and Angier but it is not evidently # clear there is a direct correlation.# Sample size of scores per inspectorinspector_samplesize = group_by(data2, INSPECTOR) %>%  summarize(number=n())# For the same reason as CITY, I used the data2 dataset rather than inspector_scores.# The most obvious variance, Thomas Jumalon, only had a sample size of 3, which# could definitely explain his noticeably lower average. Since he only inspected# 3 restaurants, it's very possible that, by chance, all three were not in perfect # condition, indicating that his scores were not due to being a harsher grader. # Question 6: # Like the previous questions, I wanted to check if there were any na values # for facility types before getting started.sum(is.na(data2$FACILITYTYPE))# There were not any, so I went ahead and identified the different types of food# facilities there were in the dataset.unique(data2$FACILITYTYPE)# There were 9.# Then, I made a new dataset with just facility type and took the average score # per facility type to see if there were any major variances facility_scores = group_by(data2, FACILITYTYPE) %>%  summarize(average_score=mean(SCORE))# It appears that restaurants, on average, receive lower scores than other# facility types. Institutional Food Service is the only other facility that has# an average score of 96, which is also the lowest average of the facility types. # Additionally, the highest score was from Elderly Nutrition Sites, at 99.25.# Question 7:# I have to filter the data to isolate restaurant data.restaurants = filter(data2, FACILITYTYPE == "Restaurant")# This pulled all data attached to just restaurants.# Now I will repeat the process for questions 1-5 for restaurant data alone# First, I want to make a histogram for the overall distribution of restaurant# scoresrestaurant_scores = restaurants$SCOREhist(restaurant_scores)# This histogram shows that the majority of restaurants received scores between # 90-100.# Now I will compare restaurant scores based on their opening year. Since I'm# using dataset "data2," I do not need to filter and remove na values. data2 was# the dataset created after removing opening year na values.# I can proceed with mutating the opening date column so that the data is in a# recognizable form.restaurants$RESTAURANTOPENDATE = ymd_hms(restaurants$RESTAURANTOPENDATE)# I then pulled the year from the opening dates since month or date were not # relevant. From there, I can analyze the relationship between inspection scores# and the year the restaurant was openedrestaurants$opening_year = floor_date(restaurants$RESTAURANTOPENDATE, unit="year")# Before plotting my data, I made a new dataset that only contained the opening# year and averaged the scores per year. This consolidated scores so there was# only one mean score per recorded year.restaurant_scores_by_opening = group_by(restaurants, opening_year) %>%  summarize(SCORE=mean(SCORE))# I plotted the data.ggplot(restaurant_scores_by_opening, aes(x=opening_year, y=SCORE)) +  geom_line()# Unlike the first opening year plot with all facility types, there is only one# unusually high average score peak in 2022 of 100. Instead, this dataset appears# to start around 1991. However, like the other plot, there is a minimum of about# 95.8 around 1997. This plot is also similar in that there are multiple # irregular peaks approximately between 1997 and 2000. Between about# 2003 and 2021, the data also levels out but the approximate scores sit around # 96.1-6.9 instead of 96.7-97.3.# Now, I will move on to scores by city in Wake County.# Since I recoded the data2 dataset already and that was how I created the# restaurants dataset, I don't need to recode again.# I made a new dataset with just cities and took the mean score for each to see # if on average, inspection scores varied per city for along restaurant facilitiesrestaurant_city_scores = group_by(restaurants, CITY) %>%  summarize(SCORE=mean(SCORE))# With the new dataset, the average scores for about half of the cities went down# at least one point. The most significant decrease was Clayton, who went down # from 96 to 93 point. Alternatively, New Hill went up to 100. The new score # distribution is more spread out with scores ranging from 93-98, all at low # frequencies/rate of instances. The only noticeable outlier is New Hill, whose # score is 100.# Next is looking at scores per inspector for just restaurants. Just like the # previous question, I made a new dataset with just the restaurant inspectors # and averaged their scoresrestaurant_inspector_scores = group_by(restaurants, INSPECTOR) %>%  summarize(average_score=mean(SCORE))# Many average scores per inspector changed around 1 point, most of them # decreasing but a fair amount increased. The distribution is an approximate # bell curve, with scores of 96 occurring 12 times. The scores with the lowest# frequencies were 94 and 98 (3), 93 (2), and 88 and 99 (1). These scores belong # to Joanne Rutkofske, Karla Crowder, Lauren Harden, John Wulffert, Greta Welch,	# Brittny Thomas, Lucy Schrum, Meghan Scott, Thomas Jumalon, and James Smith,# respectively. Low frequencies of these scores indicate variance from the typical,# average score.# Lastly, I will look at sample size of each analysis.# Sample size per opening year of restaurantsrest_year_ssize = group_by(restaurants, opening_year) %>%  summarise(number=n())# Excluding 1991 which has a rather high sample size, sample size's are very low # until about 1999, where a small increase occurs. In 2006, sample sizes begin # increasing by 20-40, until 2013 where a significant increase occurs. 2013-2021 # sample sizes are in the triple digits. 2022 is an exception with a sample size # of 1. I think this pattern aligns with the graph. The two obvious outliers have# sample sizes dissimilar to the neighboring years. Additionally, the slow # leveling out of average scores similarly follows the increases in sample size.# Sample size of restaurants' citiesrest_cities_ssize = group_by(restaurants, CITY) %>%  summarise(number=n())# Both outliers, Clayton and New Hill have a sample size of 1. However, there are# significant differences among sample sizes of all cities, indicating that# variances cannot be definitively explained by sample size.# Sample size of restaurant inspectorsrest_inspector_ssize = group_by(restaurants, INSPECTOR) %>%  summarise(number=n())# Only about a quarter of inspectors whose scores occurred at a low frequency also # had a very small sample size. Another quarter had small-medium sample size and # the others had large sample sizes of, for example, 92, 111, and 86. This # indicates that sample size is not necessarily indicative of scores and that# there may be some variances among inspectors.